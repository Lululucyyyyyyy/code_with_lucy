{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the tensorflow tutorial! We will try building a binary classifier. But first, you will need to know a bit more about neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some new libraries needed:\n",
    "* [numpy](https://docs.scipy.org/doc/numpy/reference/)\n",
    "* [matplotlib](https://matplotlib.org/3.1.1/contents.html) - for plotting graphs\n",
    "* [scipy](https://www.scipy.org/) - for importing images\n",
    "* [PIL](https://pillow.readthedocs.io/) - Python Imaging Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Kaggle Cat and Dog dataset to create a binary classification model. This model will identify cats and dogs. To do this, let's first load the dataset. We will define a function to load the data, and for each image we will resize it to 224 by 224 pixels and convert it into a numpy array. \n",
    "Optional: You can print the array to see what the data looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_images(globpath, my_list, my_labels, num):\n",
    "    for image in globpath:\n",
    "        with open(image, 'rb') as file:\n",
    "            img = Image.open(file)\n",
    "            img = img.resize((128, 128)) #this is the size we want the image to be in\n",
    "            np_img = np.array(img)       #converting this image into a numpy array\n",
    "            my_list.append(np_img)\n",
    "            my_labels.append(num)\n",
    "\n",
    "def load_test_images(globpath, my_list, my_labels, num):\n",
    "    for image in globpath:\n",
    "        with open(image, 'rb') as file:\n",
    "            img = Image.open(file)\n",
    "            img = img.resize((128, 128)) #this is the size we want the image to be in\n",
    "            np_img = np.array(img)       #converting this image into a numpy array\n",
    "            my_list.append(np_img)\n",
    "            my_labels.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the training set\n",
    "training_set = []\n",
    "training_labels = []\n",
    "load_images(glob.glob(\"../cat-and-dog/training_set/cats/*.jpg\"), training_set, training_labels, 0)\n",
    "load_images(glob.glob(\"../cat-and-dog/training_set/dogs/*.jpg\"), training_set, training_labels, 1)\n",
    "print(len(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test set\n",
    "test_set = []\n",
    "test_labels = []\n",
    "load_test_images(glob.glob(\"../cat-and-dog/test_set/cats/*.jpg\"), test_set, test_labels, 0)\n",
    "load_test_images(glob.glob(\"../cat-and-dog/test_set/dogs/*.jpg\"), test_set, test_labels, 1)\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly shuffle the dataset\n",
    "p = np.random.permutation(len(training_labels))\n",
    "X_train_orig = np.array(training_set)[p]\n",
    "Y_train = np.expand_dims(np.array(training_labels)[p], axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(test_labels))\n",
    "X_test_orig = np.array(test_set)[p]\n",
    "Y_test = np.expand_dims(np.array(test_labels)[p], axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the images, 255 is the maximun RBG value\n",
    "X_train = X_train_orig.reshape(X_train_orig.shape[0], -1).T / 255\n",
    "X_test = X_test_orig.reshape(X_test_orig.shape[0], -1).T /255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a training set of X_train and Y_train, as well as a test set of X_test and Y_test. Remember that these are all numpy arrays, and for X, they have dimensions (*hint* which is a tuple) of (m, num_pixels, num_pixels, 3), and for Y, a dimension of (m,). m is the number of training examples. Try finding the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START ###\n",
    "m_train = np.shape(X_train)[0]\n",
    "m_test = np.shape(X_test)[0]\n",
    "num_px = np.shape(X_train_orig)[1]\n",
    "### END ###\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(X_train.shape))\n",
    "print (\"train_set_y shape: \" + str(Y_train.shape))\n",
    "print (\"test_set_x shape: \" + str(X_test.shape))\n",
    "print (\"test_set_y shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "Number of training examples: m_train = 49152\n",
    "Number of testing examples: m_test = 150528\n",
    "Height/Width of each image: num_px = 2000\n",
    "Each image is of size: (2000, 2000, 3)\n",
    "train_set_x shape: (49152, 2000)\n",
    "train_set_y shape: (2000,)\n",
    "test_set_x shape: (150528, 400)\n",
    "test_set_y shape: (400,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Let's Build the Functions\n",
    "Now that we have our data loaded, we can start building the functions needed to create a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Helper Function\n",
    "Use your code from the first notebook to implement a sigmoid activation function. The backwards function should be the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$sigmoid = 1 / (1 + e^x)$$\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ### START ###\n",
    "    s = None\n",
    "    ### END ###\n",
    "    return s, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    ### START ###\n",
    "    s = None\n",
    "    ### END ###\n",
    "    return s, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backwards(dA, cache):\n",
    "    ### START ###\n",
    "    Z = cache\n",
    "    s = None\n",
    "    dZ = dA * s * (1-s)\n",
    "    ### END ###\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backwards(dA, cache):\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    ### START ###\n",
    "    Z = cache\n",
    "    dZ = None\n",
    "    ### END ###\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(sigmoid(np.array([1, 2]))))\n",
    "print(str(relu(np.array([1, -1]))))\n",
    "print(str(sigmoid_backwards(np.array([1, -1]), np.array([[0, 1]]))))\n",
    "print(str(relu_backwards(np.array([1, -1]), np.array([[0, 1]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be\n",
    "```\n",
    "(array([0.73105858, 0.88079708]), array([1, 2]))\n",
    "(array([1, 0]), array([ 1, -1]))\n",
    "[[ 0.25       -0.19661193]]\n",
    "[ 1 -1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize Parameters\n",
    "Now we're going to create a function that initializes the weight (W) randomly and the bias (b) as zeros. *hint: Numpy has functions that can help you with these* This function should return a dictionary parameters with the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START ###\n",
    "        parameters['W' + str(l)] = None\n",
    "        parameters['b' + str(l)] = None\n",
    "        ### END ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_ = initialize_parameters([1,2,3])\n",
    "print(parameters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should return\n",
    "```\n",
    "{'W1': array([[0.01788628],\n",
    "       [0.0043651 ]]), 'b1': array([[0.],\n",
    "       [0.]]), 'W2': array([[ 0.00096497, -0.01863493],\n",
    "       [-0.00277388, -0.00354759],\n",
    "       [-0.00082741, -0.00627001]]), 'b2': array([[0.],\n",
    "       [0.],\n",
    "       [0.]])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Forward Propogation\n",
    "Next, you will implement forward propogation. This is the forward pass, where the input (X) will get multiplied by the weights (w) and added to the bias (b), then go through the activation function to produce an output.\n",
    "* get the input $X = input$\n",
    "* forward prop $A = sigmoid(w^TX + b)$ \n",
    "* calculate loss $J = \\frac{-1}{m} $ $\\sum_{i=1}^{m}y^{(i)}log(a^{(i)})+(1-y^{(i)})log(1-a^{(i)})$\n",
    "The cost function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    ### START ###\n",
    "    Z = None\n",
    "    ### END ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1.23, 2.34]])\n",
    "W = np.array([[5.67], [6.78]])\n",
    "b = 3\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "```\n",
    "Z = [[ 9.9741 16.2678]\n",
    " [11.3394 18.8652]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're goint to build a combined function. It should first calculate the forward prop, then the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        ### START ###\n",
    "        Z, linear_cache = None\n",
    "        A, activation_cache = None\n",
    "        ### END ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        ### START ### \n",
    "        Z, linear_cache = None\n",
    "        A, activation_cache = None\n",
    "        ### END ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev = np.array([[1, 2, 3]])\n",
    "W = np.array([[1.12], [2.23], [3.34]])\n",
    "b = 5.67\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "With sigmoid: A = [[0.9988763  0.99963308 0.99988025]\n",
    " [0.99962939 0.99996014 0.99999571]\n",
    " [0.99987783 0.99999567 0.99999985]]\n",
    "With ReLU: A = [[ 6.79  7.91  9.03]\n",
    " [ 7.9  10.13 12.36]\n",
    " [ 9.01 12.35 15.69]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use this function to create the entire forward pass of a L-layer model. The layers should all use the linear_activation_forward() function. The model should return the last value (AL) and a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2# number of layers in the neural network\n",
    "    \n",
    "    # Implement linear -> relu (L-1) times. Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START ### \n",
    "        A, cache = None\n",
    "        caches.append(cache)\n",
    "        ### END C###\n",
    "    \n",
    "    # Implement linear -> sigmoid. Add \"cache\" to the \"caches\" list.\n",
    "    ### START ### (≈ 2 lines of code)\n",
    "    AL, cache = None\n",
    "    caches.append(cache)\n",
    "    ### END ###\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(2).reshape((1, 2))\n",
    "AL, caches = L_model_forward(X, parameters_)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "```\n",
    "AL = [[0.5        0.49998398]\n",
    " [0.5        0.49998372]\n",
    " [0.5        0.49998946]]\n",
    "Length of caches list = 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cost Function\n",
    "Now we will calculate the cost, aka how wrong the model is. We are going to use [cross entropy](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html).\n",
    "* cost = $ −(y \\log (p) + (1−y) \\log(1−p))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START ###\n",
    "    cost = None\n",
    "    ### END ###\n",
    "    \n",
    "    cost = np.squeeze(cost) \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[0.567], [0.234]])\n",
    "AL = np.array([[0.345], [0.456]])\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost should be [[0.78661854 0.70885683]\n",
    " [0.57313529 0.65009684]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Backward Propogation\n",
    "Next, you will implement backwards propogation, where the derivatives will be calculated. These are the equations that you will be using (somehow calculated with calculus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START ###\n",
    "    dW = None\n",
    "    db = None\n",
    "    dA_prev = None\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    ### END ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will combine this function with the functions sigmoid_backwards() and relu_backwards() to create the function Linear_activation_backwards(). This function should first calculate dZ using the backwards activation functions, then the other values accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START ###\n",
    "        dZ = None\n",
    "        dA_prev, dW, db = None\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = None\n",
    "        dA_prev, dW, db = None\n",
    "        ### END ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAL = np.array([[0.026, 0.888], [0.987, 0.223], [0.234, 0.567]])\n",
    "linear_activation_cache = caches[1]\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "```\n",
    "sigmoid:\n",
    "dA_prev = [[-7.26586821e-04 -5.77055938e-05]\n",
    " [-1.36329020e-03 -5.22350539e-03]]\n",
    "dW = [[0.         0.00054266]\n",
    " [0.         0.0023284 ]\n",
    " [0.         0.00083255]]\n",
    "db = [[0.11425 ]\n",
    " [0.15125 ]\n",
    " [0.100125]]\n",
    "\n",
    "relu:\n",
    "dA_prev = [[-0.00290635 -0.00023082]\n",
    " [-0.00545316 -0.02089402]]\n",
    "dW = [[0.         0.00217063]\n",
    " [0.         0.00931359]\n",
    " [0.         0.0033302 ]]\n",
    "db = [[0.457 ]\n",
    " [0.605 ]\n",
    " [0.4005]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to combine these functions to create the L_model_backwards() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Computing the cost\n",
    "    ### START ###\n",
    "    dAL = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer sigmoid -> linear grads. \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer relu -> linear grads\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Update Parameters (aka optimization)\n",
    "Now that you have the gradients for each parameter, it's time to update the parameters. The update rule is: <br>\n",
    "$ \\theta = \\theta - \\alpha \\text{ } d\\theta$ <br>\n",
    "$\\alpha $ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    ### START ### \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_ = {'W1': np.array([1.234, 3.567]), 'W2': np.array([2.345, 4.678]), 'b1': 5.000, 'b2': 10.100}\n",
    "grads = {'dW1': np.array([0.056, 0.345]), 'dW2': np.array([0.221, 1.002]), 'db1': 0.500, 'db2': 0.400}\n",
    "parameters_ = update_parameters(parameters_, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters_[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters_[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters_[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters_[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be\n",
    "```\n",
    "W1 = [1.1784 3.4825]\n",
    "b1 = 5.0\n",
    "W2 = [2.2829 4.5378]\n",
    "b2 = 10.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Model\n",
    "Now that we have all the functions, we can combine them to create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [49152, 50, 500, 14, 1] #4 layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.0075, epochs = 3000, print_cost=False):\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    ### START ###\n",
    "    #Initialize parameters\n",
    "    my_parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, epochs):\n",
    "        # Forward propagation: linear -> relu (L-1) time, then linear -> sigmoid.\n",
    "        AL, caches = L_model_forward(X, my_parameters)\n",
    "        \n",
    "        # compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        #back prop\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # update params\n",
    "        my_parameters = update_parameters(my_parameters, grads, learning_rate)\n",
    "        ### END ###\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 50 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return my_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parameters = model(X_train, Y_train, layers_dims, epochs = 500, learning_rate = 0.001, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take around 5 minutes. If your initial cost is not close to 0.702162, then there is something wrong. Your final cost should be 0.668156. Let's run this through the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, Y, parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    P = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    total = 0\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(len(probas[0])):\n",
    "        if probas[0][i] > 0.5:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        if label == Y[0][i]:\n",
    "            total = total + 1\n",
    "    accuracy = total / m\n",
    "    \n",
    "    print(\"Accuracy: \"  + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(X_test, Y_test, my_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the accuracy is not very high. We will improve this in later assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test with your own image\n",
    "Now you can try testing with your own image! Just save the image into this folder. First, we define a predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can imput your own image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "classes = ['dog', 'cat']\n",
    "my_image = \"my_image.jpg\" # change this to the name of your image file \n",
    "my_label_y = [0] # the true class of your image (1 -> cat, 0 -> dog)\n",
    "## END CODE HERE ##\n",
    "\n",
    "fname = \"images/\" + my_image\n",
    "with open(my_image, 'rb') as file:\n",
    "    image = np.array(Image.open(file))\n",
    "    my_image = np.array(Image.fromarray(image).resize((128,128))).reshape((num_px*num_px*3,1))\n",
    "    my_image = my_image/255.\n",
    "    my_predicted_image = predict(my_image, my_label_y, my_parameters)\n",
    "if my_predicted_image == [[0.]]:\n",
    "    num = 0\n",
    "else:\n",
    "    num = 1\n",
    "plt.imshow(image)\n",
    "print ((\"y = \" + str(num)) + \", your L-layer model predicts a \\\"\" + classes[num] + \"\\\" picture.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing this tutorial! Now you should be familiar with the different steps involved in a neural network. Notice how the cost is still pretty high. We can improve our algorithm by using techniques such as minibatches, dropout, and regularization. Let's continue to the next assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
