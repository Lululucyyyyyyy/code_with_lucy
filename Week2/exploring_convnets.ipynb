{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "Welcome! In this notebook, we will explore the different components of convolutional neural networks (or convnets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "First things first, let's import the basic libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Components\n",
    "For convnets, there are a few basic layers: the convolutional layer and the max pool layer. To break it down further, we padd the image before the conv layer, then we define a conv \"window\", then we do the actual forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutional Layer\n",
    "### 3.1 Zero Padding\n",
    "Without further to do, let's get started! So an image can be converted into a numpy array like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![watermelon](watermelon.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('watermelon.jpeg')\n",
    "my_np_image = np.array(image)\n",
    "print(my_np_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what we mean by padding is simply adding a layer of 0's around the image, so that we don't loose \"valuable\" information on the edges.\n",
    "You can use the function ```np.pad()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(image, pad):\n",
    "    padded = None\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "my_image = np.random.randn(3, 2, 2, 2)\n",
    "padded_image = zero_pad(my_image, 2)\n",
    "print(my_image.shape)\n",
    "print(padded_image.shape)\n",
    "print(my_image[1][1])\n",
    "print(padded_image[1][1])\n",
    "\n",
    "fig, axarr = plt.subplots(1,2)\n",
    "axarr[0].set_title('my_image')\n",
    "axarr[0].imshow(my_image[1,:,:,1])\n",
    "axarr[1].set_title('padded_image')\n",
    "axarr[1].imshow(padded_image[1,:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "(3, 2, 2, 2)\n",
    "(3, 6, 6, 2)\n",
    "[[ 0.0169049  -0.51498352]\n",
    " [ 0.24450929 -0.18931261]]\n",
    "[[0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convolutional Layer\n",
    "So now we'll try implementing our own conv layer. It may seem very complicated, but try thinking about what each line of code actually DOES, and it will start coming together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    #Element-wise product between a_slice, W\n",
    "    s = None\n",
    "    #Sum over all entries of volume s\n",
    "    Z = None\n",
    "    #Add bias b to Z. Cast b to a float()\n",
    "    Z = None\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(2, 2, 4)\n",
    "W = np.random.randn(2, 2, 4)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "Z = -1.7334289119940802\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will try implementing a forward pass for the conv layer. Use the equations below to help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ nH = (nH\\_prev - f + 2xpad) / stride + 1 $$\n",
    "$$ nW = (nH\\_prev - f + 2xpad) / stride + 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, stride = 2, pad = 2):\n",
    "    \n",
    "    #Get the dimensions of A_prev\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = None\n",
    "    \n",
    "    #Compute the dimensions using the formulas above\n",
    "    n_H = None\n",
    "    n_W = None\n",
    "    \n",
    "    #Get W's shape for parameters\n",
    "    (f, f, n_C_prev, n_C) = None\n",
    "    \n",
    "    #Initialize an output shape\n",
    "    Z = None\n",
    "    \n",
    "    #Padd the array\n",
    "    A_prev_pad = None\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = None\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = None\n",
    "                    vert_end = None\n",
    "                    horiz_start = None\n",
    "                    horiz_end = None\n",
    "                    \n",
    "                    a_slice_prev = None\n",
    "                    \n",
    "                    Z[i, h, w, c] = None\n",
    "    \n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 5, 5, 4)\n",
    "W = np.random.randn(5, 5, 4, 2)\n",
    "b = np.random.randn(1, 1, 1, 2)\n",
    "Z, cache = conv_forward(A_prev, W, b)\n",
    "\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "print(\"cache[2][0][1][9] =\", cache[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "Z's mean = -1.424723284740757\n",
    "Z[3,2,1] = [-8.39966421  4.78707002]\n",
    "cache[2][0][1][9] = [ 1.0388246   2.18697965  0.44136444 -0.10015523]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Max Pool Layer\n",
    "As you have read already, the max pool layer takes the maximum value of a window and stores it into a new volume of \"max values\". It basically shrinks the data to make it easier to calculate.\n",
    "### 4.1 Building the Layer\n",
    "You will use these 3 equations to build the max pool layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ nH = (nH\\_prev - f) / stride + 1 $$\n",
    "$$ nW = (nW\\_prev - f) / stride + 1 $$\n",
    "$$ nC = nC\\_prev $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_forwards(A_prev, stride = 2, f = 2):\n",
    "    #Get the dimensions of A_prev\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = None\n",
    "    \n",
    "    #Compute the dimensons using the equations above\n",
    "    n_H = None\n",
    "    n_W = None\n",
    "    n_C = None\n",
    "    \n",
    "    #Initialize the output matrix\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    #find the corners\n",
    "                    vert_start = None\n",
    "                    vert_end = None\n",
    "                    horiz_start = None\n",
    "                    horiz_end = None\n",
    "                    \n",
    "                    #create a slice for the ith training example\n",
    "                    a_prev_slice = None\n",
    "                    \n",
    "                    #\"pool\" the slice (hint: use np.max)\n",
    "                    A[i, h, w, c] = None\n",
    "                    \n",
    "    cache = (A_prev)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "A_prev = np.random.randn(3, 3, 3, 3) #(training examples, height, width, channels)\n",
    "\n",
    "A, cache = max_pool_forwards(A_prev, stride=2, f=2)\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "A = [[[[1.33186404 0.82145535 1.48127781]]]\n",
    "\n",
    "\n",
    " [[[0.48668927 0.58680631 1.07080136]]]\n",
    "\n",
    "\n",
    " [[[0.36062884 0.35325398 1.51572304]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling backwards will be a lot more complicated, so we won't be going over that. However, as long as you get the concept of max pooling, you are on track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats!\n",
    "You've completed this assignment. Next, we will see how to build a convnet using tf keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
